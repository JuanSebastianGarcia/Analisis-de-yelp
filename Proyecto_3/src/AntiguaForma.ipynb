{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto #3 #\n",
    "El proyecto 3 consiste en hacer una pequeÃ±a practica para notar las diferencias entre el procesamiento basico que se ha trabajado en los 2 proyectos anteriores y el procesamiento de datos en paralelo usando pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente, la forma en las que extraiamos las n palabras mas relevantes de un conjunto de informacion, usabamos la siguiente manera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos el conjunto de datos que sera procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/prueba.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#se accede al archivo libros\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/prueba.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m archivo:\n\u001b[0;32m      3\u001b[0m     libros\u001b[38;5;241m=\u001b[39marchivo\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(libros)\n",
      "File \u001b[1;32mc:\\Users\\MARTHA ISABEL VILLAM\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\proyecto_de_yelp-j_kxu5WQ-py3.12\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/prueba.txt'"
     ]
    }
   ],
   "source": [
    "#se accede al archivo libros\n",
    "with open('../data/prueba.txt','r',encoding='utf-8') as archivo:\n",
    "    libros=archivo.read()\n",
    "\n",
    "    print(libros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues de tener el archivo cargado, vamos a hacer el proceso tradicional para poder calcular las 20 palabras mas importantes.\n",
    "procederemos un pre-procesmiento para dejar listos los datos. vamos a eliminar espacios en el documento, ademas de todo tipo de caracteres que no sean alfanumericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los caracteres epeciales fueron eliminados:  The Project Gutenberg eBook of The Declaration of Independence of the United States of America\n",
      "\n",
      "    \n",
      "\n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "\n",
      "whatsoever You may copy it give it away or reuse it under the terms\n",
      "\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "\n",
      "at wwwgutenbergorg If you are not located in the United States\n",
      "\n",
      "you will have to check the laws of the country where you are located\n",
      "\n",
      "before using this eBook\n",
      "\n",
      "\n",
      "\n",
      "Title The Declaration of Independence of the United States of America\n",
      "\n",
      "\n",
      "\n",
      "Author Thomas Jefferson\n",
      "\n",
      "\n",
      "\n",
      "Release date December 1 1971 eBook 1\n",
      "\n",
      "                Most recently updated January 1 2021\n",
      "\n",
      "\n",
      "\n",
      "Language English\n"
     ]
    }
   ],
   "source": [
    "import re #libreria para trabajar con expresiones regulares\n",
    "\n",
    "#almacenamos los datos sin caracteres especiales\n",
    "libros = re.sub(r'[^a-zA-Z0-9\\s]', '', libros)\n",
    "\n",
    "print(f'Los caracteres epeciales fueron eliminados:  {libros}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de eliminar los stopwords, vamos a tokenizar el texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La lista de tokens se puede ver a continuacion:  ['The', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'Declaration', 'of', 'Independence', 'of', 'the', 'United', 'States', 'of', 'America', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'You', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'If', 'you', 'are', 'not', 'located', 'in', 'the', 'United', 'States', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'eBook', 'Title', 'The', 'Declaration', 'of', 'Independence', 'of', 'the', 'United', 'States', 'of', 'America', 'Author', 'Thomas', 'Jefferson', 'Release', 'date', 'December', '1', '1971', 'eBook', '1', 'Most', 'recently', 'updated', 'January', '1', '2021', 'Language', 'English']\n"
     ]
    }
   ],
   "source": [
    "import nltk #libreria para tokenizar \n",
    "\n",
    "#tokenizar los libros\n",
    "tokens = nltk.word_tokenize(libros)\n",
    "\n",
    "print(f'La lista de tokens se puede ver a continuacion:  {tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora, con las palabras tokenizadas, si se puede eliminar los stop words. los stopwords son palabras que no generan ningun tipo de relevancia al proceso\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se eliminaron los stopwords:  ['Project', 'Gutenberg', 'eBook', 'Declaration', 'Independence', 'United', 'States', 'America', 'ebook', 'use', 'anyone', 'anywhere', 'United', 'States', 'parts', 'world', 'cost', 'almost', 'restrictions', 'whatsoever', 'may', 'copy', 'give', 'away', 'reuse', 'terms', 'Project', 'Gutenberg', 'License', 'included', 'ebook', 'online', 'wwwgutenbergorg', 'located', 'United', 'States', 'check', 'laws', 'country', 'located', 'using', 'eBook', 'Title', 'Declaration', 'Independence', 'United', 'States', 'America', 'Author', 'Thomas', 'Jefferson', 'Release', 'date', 'December', '1', '1971', 'eBook', '1', 'recently', 'updated', 'January', '1', '2021', 'Language', 'English']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords #lista de stop words ya definida\n",
    "\n",
    "#almacenar las stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#eliminamos los stopwords\n",
    "tokens_clean = [token for token in tokens if not str(token).lower() in stopwords]\n",
    "\n",
    "print(f'Se eliminaron los stopwords:  {tokens_clean}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, vamos a generar una matriz de frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La siguiente es una matriz de frecuencia:    (0, 30)\t1.0\n",
      "  (1, 18)\t1.0\n",
      "  (2, 15)\t1.0\n",
      "  (3, 14)\t1.0\n",
      "  (4, 20)\t1.0\n",
      "  (5, 39)\t1.0\n",
      "  (6, 35)\t1.0\n",
      "  (7, 3)\t1.0\n",
      "  (8, 15)\t1.0\n",
      "  (9, 41)\t1.0\n",
      "  (10, 4)\t1.0\n",
      "  (11, 5)\t1.0\n",
      "  (12, 39)\t1.0\n",
      "  (13, 35)\t1.0\n",
      "  (14, 29)\t1.0\n",
      "  (15, 44)\t1.0\n",
      "  (16, 10)\t1.0\n",
      "  (17, 2)\t1.0\n",
      "  (18, 33)\t1.0\n",
      "  (19, 43)\t1.0\n",
      "  (20, 27)\t1.0\n",
      "  (21, 9)\t1.0\n",
      "  (22, 17)\t1.0\n",
      "  (23, 7)\t1.0\n",
      "  (24, 34)\t1.0\n",
      "  :\t:\n",
      "  (37, 24)\t1.0\n",
      "  (38, 11)\t1.0\n",
      "  (39, 26)\t1.0\n",
      "  (40, 42)\t1.0\n",
      "  (41, 15)\t1.0\n",
      "  (42, 38)\t1.0\n",
      "  (43, 14)\t1.0\n",
      "  (44, 20)\t1.0\n",
      "  (45, 39)\t1.0\n",
      "  (46, 35)\t1.0\n",
      "  (47, 3)\t1.0\n",
      "  (48, 6)\t1.0\n",
      "  (49, 37)\t1.0\n",
      "  (50, 22)\t1.0\n",
      "  (51, 32)\t1.0\n",
      "  (52, 12)\t1.0\n",
      "  (53, 13)\t1.0\n",
      "  (55, 0)\t1.0\n",
      "  (56, 15)\t1.0\n",
      "  (58, 31)\t1.0\n",
      "  (59, 40)\t1.0\n",
      "  (60, 21)\t1.0\n",
      "  (62, 1)\t1.0\n",
      "  (63, 23)\t1.0\n",
      "  (64, 16)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer #\n",
    "\n",
    "#hacemos una vectorizacion sobre los tokens \n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#aplicamos el proceso de vectorizacion\n",
    "matriz_frecuencia = vectorizer.fit_transform(tokens_clean)\n",
    "\n",
    "print(f'La siguiente es una matriz de frecuencia:  {matriz_frecuencia}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y extraemos las 20 palabras mas importantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ebook', 'states', 'united', 'america', 'declaration', 'gutenberg', 'independence', 'located', 'project', '1971', '2021', 'almost', 'anyone', 'anywhere', 'author', 'away', 'check', 'copy', 'cost', 'country']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd #libreria para el manejo de datos o dataframe\n",
    "\n",
    "#obtener las palabras vectorizadas y la frecuencia\n",
    "frecuentia= matriz_frecuencia.toarray().sum(axis=0)\n",
    "palabras=vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "#generamos un dataframe con todas las palabras\n",
    "data = pd.DataFrame({'Palabra': palabras,'Frecuencia':frecuentia})\n",
    "\n",
    "#extraemos las 20 palabras mas frecuentes\n",
    "mas_repetidas = data.nlargest(20,'Frecuencia')['Palabra'].tolist()\n",
    "\n",
    "print(mas_repetidas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto_de_yelp-j_kxu5WQ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
